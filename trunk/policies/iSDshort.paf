#<?cfg paf policy ?>
#
# Pipeline Layer Policy
#

# this part is used by the orchestration layer to determine if
# this pipeline can run on a particular platform or with a particular
# database server
#
requires: {
   database: {
      type:  MySQL

   }

   platform: {
      # minNodeCount:  4
      # minCoresPerNode:  8
      minCoreCount:  2
   }
}

# this part is used by the orchestration layer to 
setup: {

   # the schema of this item is determined by the database type given 
   # above in requires.database.type.  
   database:  {
      # If not provided, this defaults to $CTRL_DC3PIPE_DIR
      # 
      # scriptRepository: /lsst/...

      script: lsstSchema4mysql.sql
      script: lsstPipelineSetup4mysql.sql
   }

   data:  {
      
   }
}

framework: {

   # the type determines the schema of the "execute" policy below
   type:  standard

   # this is the file that should be sourced to set the environment on
   # the head node where the pipeline is executed.  The file path component
   # can be represented as $ENVVAR which will be replaced with the
   # value of the environment variable with the name ENVVAR.
   # 
   environment: "$CTRL_DC3PIPE_DIR/etc/setup.sh"

   # this is the execution script that we will use to start the
   # pipeline on the head node of the platform.  The file path component
   # can be represented as $ENVVAR which will be replaced with the
   # value of the environment variable with the name ENVVAR.
   # 
   exec:  "$CTRL_DC3PIPE_DIR/bin/launchPipeline.sh"
}

# the contents of this item is passed to the harness to configure the
# pipeline at launch time.
# 
execute: {

   # executionMode: "oneloop" ,
   # shutdownTopic: "triggerShutdownEvent" 

   localLogMode:  true
   shutdownTopic:  triggerSundownA

   ##
   # Stage configuration
   
   # Stage 2: load input data (science and template images) into memory
   appStage: {
      stageName: "lsst.pex.harness.IOStage.InputStage"
      eventTopic: "triggerVisitEvent"  
      stagePolicy: "iSDshort/input_policy.paf"
   }
   
   # Stage 4: persist the per-visit metadata
   appStage: {
      stageName: "lsst.pex.harness.IOStage.OutputStage"
      eventTopic: "None"
      stagePolicy: "iSDshort/metadataOutput_policy.paf"
   }
   
   # Stage 5: persist the Exposure with its metadata
   appStage: {
      stageName: "lsst.pex.harness.IOStage.OutputStage"
      eventTopic: "None"
      stagePolicy: "iSDshort/exposureOutput_policy.paf"
   }
   
   # Stage 7: write out the difference image
   appStage: {
      stageName: "lsst.pex.harness.IOStage.OutputStage"
      eventTopic: "None"
      stagePolicy: "iSDshort/subtractOutput_policy.paf"
   }
   
   # Stage 9: write out the table of detected sources to the database
   appStage: {
      stageName: "lsst.pex.harness.IOStage.OutputStage"
      eventTopic: "None"
      stagePolicy: "iSDshort/detectOutput_policy.paf"
   }
   
   # Stage 10: send an event to the association pipeline indicating that 
   #           new detections are available
   appStage: {
      stageName: "lsst.pex.harness.EventStage.EventStage"
      eventTopic: "None"
      stagePolicy: "iSDshort/associationEvent_policy.paf"
   }


   # the rest items will get overridden by the policy passed from the
   # orchestration layer.  It appears here as default values for when
   # the pipeline is launch directly via pipeline harness rather than
   # via the orchestration layer (e.g. for testing purposes).  

   eventBrokerHost:  lsst8.ncsa.uiuc.edu

   dir: {

      # the default root directory all files read or written by pipelines
      # deployed on this platform.  
      # This can be overriden by any of the "named role" directories below.
      #
      defaultRoot:  .

      runDirPattern: "%(runid)s"

      # These indicate the directory that should be used for a named purpose.
      # If relative paths are given, the resulting directory will be relative
      # to the default run directory (determined by defaultRoot and the 
      # runDirPattern).  These can be given as patterns specified in the same 
      # format as runDirPattern.  (If a directory is given as an absolute path,
      # using a pattern is recommended in order to distinguish between different 
      # production runs.)
      #
      work:     .    # the working directory, where the pipeline is started
      input:    .    # the directory to cache/find input data
      output:   .    # the directory to write output data
      update:   .    # the directory where updatable data is deployed
      scratch:  .    # a directory for temporary files that may be deleted 
                     #   upon completion of the pipeline

   }
}
